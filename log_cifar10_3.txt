Вс дек 24 01:34:12 MSK 2017
2017-12-24 01:34:18.355796: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-24 01:34:18.355828: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-24 01:34:18.355835: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-24 01:34:18.355839: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-24 01:34:18.355843: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-24 01:34:18.451876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-12-24 01:34:18.452506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 950M
major: 5 minor: 0 memoryClockRate (GHz) 0.928
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.76GiB
2017-12-24 01:34:18.452521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-12-24 01:34:18.452527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-12-24 01:34:18.452534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)
Initializing dataset...
Initializing objects...
Fig size: 
(5, 5)
Initializing model...
________________________________________________________________________________________________________________________
Layer (type)                           Output Shape               Param #       Connected to                            
========================================================================================================================
input_3 (InputLayer)                   (None, 2)                  0                                                     
________________________________________________________________________________________________________________________
input_2 (InputLayer)                   (None, 10)                 0                                                     
________________________________________________________________________________________________________________________
concatenate_1 (Concatenate)            (None, 12)                 0             input_3[0][0]                           
                                                                                input_2[0][0]                           
________________________________________________________________________________________________________________________
dense_1 (Dense)                        (None, 4096)               53248         concatenate_1[0][0]                     
________________________________________________________________________________________________________________________
dropout_1 (Dropout)                    (None, 4096)               0             dense_1[0][0]                           
________________________________________________________________________________________________________________________
reshape_1 (Reshape)                    (None, 8, 8, 64)           0             dropout_1[0][0]                         
________________________________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)         (None, 16, 16, 64)         0             reshape_1[0][0]                         
________________________________________________________________________________________________________________________
conv2d_1 (Conv2D)                      (None, 16, 16, 64)         102464        up_sampling2d_1[0][0]                   
________________________________________________________________________________________________________________________
dropout_2 (Dropout)                    (None, 16, 16, 64)         0             conv2d_1[0][0]                          
________________________________________________________________________________________________________________________
conv2d_2 (Conv2D)                      (None, 16, 16, 32)         18464         dropout_2[0][0]                         
________________________________________________________________________________________________________________________
dropout_3 (Dropout)                    (None, 16, 16, 32)         0             conv2d_2[0][0]                          
________________________________________________________________________________________________________________________
conv2d_3 (Conv2D)                      (None, 16, 16, 32)         9248          dropout_3[0][0]                         
________________________________________________________________________________________________________________________
dropout_4 (Dropout)                    (None, 16, 16, 32)         0             conv2d_3[0][0]                          
________________________________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)         (None, 32, 32, 32)         0             dropout_4[0][0]                         
________________________________________________________________________________________________________________________
conv2d_4 (Conv2D)                      (None, 32, 32, 3)          2403          up_sampling2d_2[0][0]                   
========================================================================================================================
Total params: 185,827
Trainable params: 185,827
Non-trainable params: 0
________________________________________________________________________________________________________________________
________________________________________________________________________________________________________________________
Layer (type)                           Output Shape               Param #       Connected to                            
========================================================================================================================
input_2 (InputLayer)                   (None, 10)                 0                                                     
________________________________________________________________________________________________________________________
input_1 (InputLayer)                   (None, 32, 32, 3)          0                                                     
________________________________________________________________________________________________________________________
repeat_vector_1 (RepeatVector)         (None, 256, 10)            0             input_2[0][0]                           
________________________________________________________________________________________________________________________
conv2d_5 (Conv2D)                      (None, 16, 16, 128)        18944         input_1[0][0]                           
________________________________________________________________________________________________________________________
reshape_2 (Reshape)                    (None, 16, 16, 10)         0             repeat_vector_1[0][0]                   
________________________________________________________________________________________________________________________
concatenate_2 (Concatenate)            (None, 16, 16, 138)        0             conv2d_5[0][0]                          
                                                                                reshape_2[0][0]                         
________________________________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)              (None, 16, 16, 138)        0             concatenate_2[0][0]                     
________________________________________________________________________________________________________________________
conv2d_6 (Conv2D)                      (None, 16, 16, 128)        159104        leaky_re_lu_1[0][0]                     
________________________________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)              (None, 16, 16, 128)        0             conv2d_6[0][0]                          
________________________________________________________________________________________________________________________
dropout_5 (Dropout)                    (None, 16, 16, 128)        0             leaky_re_lu_2[0][0]                     
________________________________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)         (None, 8, 8, 128)          0             dropout_5[0][0]                         
________________________________________________________________________________________________________________________
conv2d_7 (Conv2D)                      (None, 8, 8, 128)          147584        max_pooling2d_1[0][0]                   
________________________________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)              (None, 8, 8, 128)          0             conv2d_7[0][0]                          
________________________________________________________________________________________________________________________
dropout_6 (Dropout)                    (None, 8, 8, 128)          0             leaky_re_lu_3[0][0]                     
________________________________________________________________________________________________________________________
flatten_1 (Flatten)                    (None, 8192)               0             dropout_6[0][0]                         
________________________________________________________________________________________________________________________
dense_2 (Dense)                        (None, 1)                  8193          flatten_1[0][0]                         
========================================================================================================================
Total params: 333,825
Trainable params: 333,825
Non-trainable params: 0
________________________________________________________________________________________________________________________
Start training model...
[2KUsing TensorFlow backend.
[2Kepoch: 0; loss: 0.6944801807403564
[2K[2Kepoch: 20; loss: 0.6920908093452454
[2K[2Kepoch: 40; loss: 0.6936184167861938
[2K[2Kepoch: 60; loss: 0.6936523914337158
[2K[2Kepoch: 80; loss: 0.6867089867591858
[2K[2Kepoch: 100; loss: 0.6771717071533203
[2K[2Kepoch: 120; loss: 0.649066686630249
[2K[2Kepoch: 140; loss: 0.5891366004943848
[2K[2Kepoch: 160; loss: 0.6116431355476379
[2K[2Kepoch: 180; loss: 0.5842670202255249
[2K[2Kepoch: 200; loss: 0.6876853704452515
[2K[2Kepoch: 220; loss: 0.6777157783508301
[2K[2Kepoch: 240; loss: 0.7408839464187622
[2K[2Kepoch: 260; loss: 0.6443594694137573
[2K[2Kepoch: 280; loss: 0.6548307538032532
[2K[2Kepoch: 300; loss: 0.5924482941627502
[2K[2Kepoch: 320; loss: 0.5672948956489563
[2K[2Kepoch: 340; loss: 0.710140585899353
[2K[2Kepoch: 360; loss: 0.6442111730575562
[2K[2Kepoch: 380; loss: 0.651273787021637
[2K[2Kepoch: 400; loss: 0.6688472032546997
[2K[2Kepoch: 420; loss: 0.6685203313827515
[2K[2Kepoch: 440; loss: 0.6449480652809143
[2K[2Kepoch: 460; loss: 0.6554828882217407
[2K[2Kepoch: 480; loss: 0.6673740148544312
Saving model at 499...
[2K[2Kepoch: 500; loss: 0.6739373207092285
[2K[2Kepoch: 520; loss: 0.6711682081222534
[2K[2Kepoch: 540; loss: 0.680397629737854
[2K[2Kepoch: 560; loss: 0.6581048965454102
[2K[2Kepoch: 580; loss: 0.6756370663642883
[2K[2Kepoch: 600; loss: 0.6720931529998779
[2K[2Kepoch: 620; loss: 0.6791427731513977
[2K[2Kepoch: 640; loss: 0.6728295087814331
[2K[2Kepoch: 660; loss: 0.6657524704933167
[2K[2Kepoch: 680; loss: 0.6588178277015686
[2K[2Kepoch: 700; loss: 0.6572978496551514
[2K[2Kepoch: 720; loss: 0.648430585861206
[2K[2Kepoch: 740; loss: 0.6315488815307617
[2K[2Kepoch: 760; loss: 0.6648516654968262
[2K[2Kepoch: 780; loss: 0.6259636878967285
[2K[2Kepoch: 800; loss: 0.6670580506324768
[2K[2Kepoch: 820; loss: 0.6722221374511719
[2K[2Kepoch: 840; loss: 0.6785876750946045
[2K[2Kepoch: 860; loss: 0.6704040765762329
[2K[2Kepoch: 880; loss: 0.682159960269928
[2K[2Kepoch: 900; loss: 0.6731178760528564
[2K[2Kepoch: 920; loss: 0.6624099016189575
[2K[2Kepoch: 940; loss: 0.6830849647521973
[2K[2Kepoch: 960; loss: 0.671582818031311
[2K[2Kepoch: 980; loss: 0.6668864488601685
Saving model at 999...
[2K[2Kepoch: 1000; loss: 0.6757418513298035
[2K[2Kepoch: 1020; loss: 0.6734892129898071
[2K[2Kepoch: 1040; loss: 0.6748300790786743
[2K[2Kepoch: 1060; loss: 0.6795635223388672
[2K[2Kepoch: 1080; loss: 0.651909589767456
[2K[2Kepoch: 1100; loss: 0.6724001169204712
[2K[2Kepoch: 1120; loss: 0.662283182144165
[2K[2Kepoch: 1140; loss: 0.6728525161743164
[2K[2Kepoch: 1160; loss: 0.651558518409729
[2K[2Kepoch: 1180; loss: 0.665837287902832
[2K[2Kepoch: 1200; loss: 0.6759664416313171
[2K[2Kepoch: 1220; loss: 0.6687777042388916
[2K[2Kepoch: 1240; loss: 0.6600863337516785
[2K[2Kepoch: 1260; loss: 0.6666637659072876
[2K[2Kepoch: 1280; loss: 0.6691911220550537
[2K[2Kepoch: 1300; loss: 0.6714441776275635
[2K[2Kepoch: 1320; loss: 0.6612167358398438
[2K[2Kepoch: 1340; loss: 0.6695181131362915
[2K[2Kepoch: 1360; loss: 0.6671863198280334
[2K[2Kepoch: 1380; loss: 0.6679975390434265
[2K[2Kepoch: 1400; loss: 0.6703284978866577
[2K[2Kepoch: 1420; loss: 0.6687761545181274
[2K[2Kepoch: 1440; loss: 0.6679890155792236
[2K[2Kepoch: 1460; loss: 0.6612191200256348
[2K[2Kepoch: 1480; loss: 0.6695554852485657
Saving model at 1499...
[2K[2Kepoch: 1500; loss: 0.66395103931427
[2K[2Kepoch: 1520; loss: 0.6711629629135132
[2K[2Kepoch: 1540; loss: 0.6658322215080261
[2K[2Kepoch: 1560; loss: 0.6623191237449646
[2K[2Kepoch: 1580; loss: 0.6645703315734863
[2K[2Kepoch: 1600; loss: 0.6652618646621704
[2K[2Kepoch: 1620; loss: 0.6679982542991638
[2K[2Kepoch: 1640; loss: 0.6754942536354065
[2K[2Kepoch: 1660; loss: 0.6705418229103088
[2K[2Kepoch: 1680; loss: 0.6596048474311829
[2K[2Kepoch: 1700; loss: 0.6680130958557129
[2K[2Kepoch: 1720; loss: 0.6770058870315552
[2K[2Kepoch: 1740; loss: 0.6716200709342957
[2K[2Kepoch: 1760; loss: 0.6676690578460693
[2K[2Kepoch: 1780; loss: 0.6643357276916504
[2K[2Kepoch: 1800; loss: 0.6672050952911377
[2K[2Kepoch: 1820; loss: 0.6647745966911316
[2K[2Kepoch: 1840; loss: 0.660140335559845
[2K[2Kepoch: 1860; loss: 0.6733918190002441
[2K[2Kepoch: 1880; loss: 0.6636593341827393
[2K[2Kepoch: 1900; loss: 0.6625598669052124
[2K[2Kepoch: 1920; loss: 0.6764339208602905
[2K[2Kepoch: 1940; loss: 0.670987069606781
[2K[2Kepoch: 1960; loss: 0.6681385040283203
[2K[2Kepoch: 1980; loss: 0.6621717810630798
Saving model at 1999...
[2K[2Kepoch: 2000; loss: 0.6602709293365479
epoch: 2020; loss: 0.6658987998962402
epoch: 2040; loss: 0.6714189052581787
epoch: 2060; loss: 0.6578389406204224
epoch: 2080; loss: 0.6776552796363831
epoch: 2100; loss: 0.6634702682495117
epoch: 2120; loss: 0.6674296855926514
epoch: 2140; loss: 0.6720011234283447
epoch: 2160; loss: 0.6626214385032654
epoch: 2180; loss: 0.6580191850662231
[2K[2Kepoch: 2200; loss: 0.6771624088287354
epoch: 2220; loss: 0.6569806933403015
epoch: 2240; loss: 0.6672788858413696
epoch: 2260; loss: 0.6663147211074829
epoch: 2280; loss: 0.6660290956497192
epoch: 2300; loss: 0.677234411239624
epoch: 2320; loss: 0.6700059175491333
epoch: 2340; loss: 0.6681769490242004
epoch: 2360; loss: 0.6658564805984497
epoch: 2380; loss: 0.6665475368499756
[2K[2Kepoch: 2400; loss: 0.664594292640686
epoch: 2420; loss: 0.6673584580421448
epoch: 2440; loss: 0.6739132404327393
epoch: 2460; loss: 0.6680097579956055
epoch: 2480; loss: 0.6646709442138672
Saving model at 2499...
epoch: 2500; loss: 0.6600765585899353
epoch: 2520; loss: 0.6651287078857422
epoch: 2540; loss: 0.6669591665267944
epoch: 2560; loss: 0.6713221669197083
epoch: 2580; loss: 0.6656574606895447
[2K[2Kepoch: 2600; loss: 0.6557375192642212
epoch: 2620; loss: 0.6633013486862183
epoch: 2640; loss: 0.6606000661849976
epoch: 2660; loss: 0.6614689826965332
epoch: 2680; loss: 0.6671196222305298
epoch: 2700; loss: 0.6649014949798584
epoch: 2720; loss: 0.6674303412437439
epoch: 2740; loss: 0.6675189733505249
epoch: 2760; loss: 0.6593226194381714
epoch: 2780; loss: 0.658682107925415
[2K[2Kepoch: 2800; loss: 0.6631496548652649
epoch: 2820; loss: 0.6499392986297607
epoch: 2840; loss: 0.6781644821166992
epoch: 2860; loss: 0.6702843904495239
epoch: 2880; loss: 0.6736756563186646
epoch: 2900; loss: 0.6628332138061523
epoch: 2920; loss: 0.6616452932357788
epoch: 2940; loss: 0.6712653040885925
epoch: 2960; loss: 0.6588798761367798
epoch: 2980; loss: 0.6616171598434448
Saving model at 2999...
[2K[2Kepoch: 3000; loss: 0.6705280542373657
epoch: 3020; loss: 0.6569482088088989
epoch: 3040; loss: 0.6580101251602173
epoch: 3060; loss: 0.6549557447433472
epoch: 3080; loss: 0.6663532257080078
epoch: 3100; loss: 0.6697957515716553
epoch: 3120; loss: 0.6663899421691895
epoch: 3140; loss: 0.6662086248397827
epoch: 3160; loss: 0.6541144251823425
epoch: 3180; loss: 0.6746521592140198
[2K[2Kepoch: 3200; loss: 0.6574631929397583
epoch: 3220; loss: 0.6589959859848022
epoch: 3240; loss: 0.6644203662872314
epoch: 3260; loss: 0.6577528119087219
epoch: 3280; loss: 0.6621986031532288
epoch: 3300; loss: 0.6652626395225525
epoch: 3320; loss: 0.6589821577072144
epoch: 3340; loss: 0.6540343761444092
epoch: 3360; loss: 0.6609611511230469
epoch: 3380; loss: 0.658848762512207
[2K[2Kepoch: 3400; loss: 0.6532491445541382
epoch: 3420; loss: 0.6460456848144531
epoch: 3440; loss: 0.6531314253807068
epoch: 3460; loss: 0.6594055891036987
epoch: 3480; loss: 0.6550271511077881
Saving model at 3499...
epoch: 3500; loss: 0.657724142074585
epoch: 3520; loss: 0.6648523211479187
epoch: 3540; loss: 0.6538258194923401
epoch: 3560; loss: 0.6621779203414917
epoch: 3580; loss: 0.6658109426498413
[2K[2Kepoch: 3600; loss: 0.6562941670417786
epoch: 3620; loss: 0.6708999276161194
epoch: 3640; loss: 0.6547741889953613
epoch: 3660; loss: 0.6545397043228149
epoch: 3680; loss: 0.6673932075500488
epoch: 3700; loss: 0.655200719833374
epoch: 3720; loss: 0.6732665300369263
epoch: 3740; loss: 0.6640982031822205
epoch: 3760; loss: 0.6650091409683228
epoch: 3780; loss: 0.6658720970153809
[2K[2Kepoch: 3800; loss: 0.6661282181739807
epoch: 3820; loss: 0.6658899188041687
epoch: 3840; loss: 0.6615412831306458
epoch: 3860; loss: 0.6561846733093262
epoch: 3880; loss: 0.657533586025238
epoch: 3900; loss: 0.6644787788391113
epoch: 3920; loss: 0.6444615125656128
epoch: 3940; loss: 0.6597279906272888
epoch: 3960; loss: 0.6587587594985962
epoch: 3980; loss: 0.6436276435852051
Saving model at 3999...
[2K[2Kepoch: 4000; loss: 0.6549608707427979
epoch: 4020; loss: 0.6637253761291504
epoch: 4040; loss: 0.6510846614837646
epoch: 4060; loss: 0.6514984369277954
epoch: 4080; loss: 0.6616419553756714
epoch: 4100; loss: 0.6552416086196899
epoch: 4120; loss: 0.6604666709899902
epoch: 4140; loss: 0.6543798446655273
epoch: 4160; loss: 0.6544607877731323
epoch: 4180; loss: 0.6438248157501221
[2K[2Kepoch: 4200; loss: 0.6466543674468994
epoch: 4220; loss: 0.679244875907898
epoch: 4240; loss: 0.6528342366218567
epoch: 4260; loss: 0.6337732076644897
epoch: 4280; loss: 0.661237359046936
epoch: 4300; loss: 0.6442772150039673
epoch: 4320; loss: 0.6525475978851318
epoch: 4340; loss: 0.6605179309844971
epoch: 4360; loss: 0.6472333669662476
epoch: 4380; loss: 0.6577327251434326
[2K[2Kepoch: 4400; loss: 0.6543475389480591
epoch: 4420; loss: 0.6691670417785645
epoch: 4440; loss: 0.6510751843452454
epoch: 4460; loss: 0.6573039889335632
epoch: 4480; loss: 0.6553469896316528
Saving model at 4499...
epoch: 4500; loss: 0.6537338495254517
epoch: 4520; loss: 0.6350587606430054
epoch: 4540; loss: 0.6352866888046265
epoch: 4560; loss: 0.6622471809387207
epoch: 4580; loss: 0.6352262496948242
[2K[2Kepoch: 4600; loss: 0.6456560492515564
epoch: 4620; loss: 0.6563026905059814
epoch: 4640; loss: 0.6371756792068481
epoch: 4660; loss: 0.6457260251045227
epoch: 4680; loss: 0.6463404297828674
epoch: 4700; loss: 0.6394075155258179
epoch: 4720; loss: 0.6508122682571411
epoch: 4740; loss: 0.6386903524398804
epoch: 4760; loss: 0.6583731174468994
epoch: 4780; loss: 0.6518986225128174
[2K[2Kepoch: 4800; loss: 0.6550817489624023
epoch: 4820; loss: 0.6422361135482788
epoch: 4840; loss: 0.6541099548339844
epoch: 4860; loss: 0.6646197438240051
epoch: 4880; loss: 0.6539709568023682
epoch: 4900; loss: 0.6414506435394287
epoch: 4920; loss: 0.6476418375968933
epoch: 4940; loss: 0.6585755944252014
epoch: 4960; loss: 0.648078203201294
epoch: 4980; loss: 0.6390336155891418
Saving model at 4999...
[2K[2Kepoch: 5000; loss: 0.6556923389434814
epoch: 5020; loss: 0.6403850317001343
epoch: 5040; loss: 0.6566070914268494
epoch: 5060; loss: 0.6521391868591309
epoch: 5080; loss: 0.6526644229888916
epoch: 5100; loss: 0.649346113204956
epoch: 5120; loss: 0.6398031711578369
epoch: 5140; loss: 0.6553716659545898
epoch: 5160; loss: 0.6589474678039551
epoch: 5180; loss: 0.6519333124160767
[2K[2Kepoch: 5200; loss: 0.6442925930023193
epoch: 5220; loss: 0.639103889465332
epoch: 5240; loss: 0.6600674390792847
epoch: 5260; loss: 0.6386544108390808
epoch: 5280; loss: 0.6675621271133423
epoch: 5300; loss: 0.6564369797706604
epoch: 5320; loss: 0.6422528028488159
epoch: 5340; loss: 0.637280285358429
epoch: 5360; loss: 0.64291912317276
epoch: 5380; loss: 0.6375619173049927
[2K[2Kepoch: 5400; loss: 0.644722044467926
epoch: 5420; loss: 0.6416031122207642
epoch: 5440; loss: 0.6351405382156372
epoch: 5460; loss: 0.6411598324775696
epoch: 5480; loss: 0.6493802070617676
Saving model at 5499...
epoch: 5500; loss: 0.6391474008560181
epoch: 5520; loss: 0.6617791652679443
epoch: 5540; loss: 0.6372858285903931
epoch: 5560; loss: 0.6548476219177246
epoch: 5580; loss: 0.6554408073425293
[2K[2Kepoch: 5600; loss: 0.6455013155937195
epoch: 5620; loss: 0.6500289440155029
epoch: 5640; loss: 0.645203173160553
epoch: 5660; loss: 0.640906035900116
epoch: 5680; loss: 0.6824744939804077
epoch: 5700; loss: 0.6414170861244202
epoch: 5720; loss: 0.6498013734817505
epoch: 5740; loss: 0.6411176323890686
epoch: 5760; loss: 0.6448250412940979
epoch: 5780; loss: 0.643189549446106
[2K[2Kepoch: 5800; loss: 0.6671236753463745
epoch: 5820; loss: 0.6490867137908936
epoch: 5840; loss: 0.6417323350906372
epoch: 5860; loss: 0.6446967124938965
epoch: 5880; loss: 0.6503645181655884
epoch: 5900; loss: 0.6469670534133911
epoch: 5920; loss: 0.6565501689910889
epoch: 5940; loss: 0.6491509675979614
epoch: 5960; loss: 0.6658515930175781
epoch: 5980; loss: 0.6420980095863342
Saving model at 5999...
Drawing images for 0...
gif done
result done
Drawing images for 1...
gif done
result done
Drawing images for 2...
gif done
result done
Drawing images for 3...
gif done
result done
Drawing images for 4...
gif done
result done
Drawing images for 5...
gif done
result done
Drawing images for 6...
gif done
result done
Drawing images for 7...
gif done
result done
Drawing images for 8...
gif done
result done
Drawing images for 9...
gif done
result done
Вс дек 24 07:08:09 MSK 2017
